%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% use formatting for item enumeration
\usepackage{enumitem}

\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Kotfis, Li, Ouyang}

\begin{document} 

\twocolumn[
\icmltitle{BroadMind: A Better Platforming Agent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Dave Kotfis}{dkotfis@seas.upenn.edu}
\icmlauthor{Zhi Li}{zhili@seas.upenn.edu}
\icmlauthor{Yesha Ouyang}{yesha@wharton.upenn.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{machine learning, reinforcement learning, deep learning}

]

\begin{abstract} 
Recent work in reinforcement learning has focused on building generalist video game agents, as opposed to focusing on a particular genre of games. We aim to build a more specialized high-performance agent focused on the more challenging genre of platform games, which has received less attention. Utilizing symbolic representations of game state, we are training fully connected Neural Q-Network agents to successfully learn to play games with long term rewards and complex dynamics.
\end{abstract} 



\section{Background}
Platform games involve a free-running avatar that jumps between suspended platforms and avoid obstacles to advance through levels in the game. As a result of the array of environments to parse and the huge decision space, these games are very difficult for learning agents to play well. We are building an agent does not have to simulatenously recognize the screen-space pixels of the game and decide optimal policies. Instead, we have decoupled these two problems, using symbolic state representations from the environment encoded for the agent. However, this symbolic representation is still very large, motivating us to utilize neural network-based Q-learning approaches. 

\section{Approach}

\subsection{Experimental Setup}

We have developed a Neural Q-Network algorithm that can be extended as a reinforcement learning agent in multiple gaming environments including Generalized Mario and the Arcade Learning Environment (ALE). We utilize the RL-Glue framework to allow our agents and experiments to be used across these environments.

RL-Glue is a socket-based API that enables reinforcement learning experiments to work with software across multiple languages \cite{Tanner09}. Is allows our experiments to connect reusable Python agents across many open source environments written in languages including C++ and Java. It also enables us to customize our experiments, such as optional game visualization, loading and saving trained policies, adjusting the game difficulty, etc.

We have started evaluating our agents in the Generalized Mario environment. This is part of the 2009 AI Competition software package and is RL-Glue compatible \cite{Togelius10}. The Generalized Mario game has a total control input space of 12, which we encode to integers. The raw state is made up of left/right/none motions, on/off for jump, and on/off for run. The observation interface provides the 2D screen position and velocity of the mario actor, as well as a 22x16 tile grid semantically describing the screen space with the location of coins, pipes, blocks, etc. Separately, it has a list of all enemy positions on-screen, with similar position and velocity information as provided about Mario. 

We have leveraged the Arcade Learning Environment, an RL-Glue compatible framework built on top of an Atari 2600 emulator \cite{bellemare13}.  It provides an API for interfacing with the raw pixels, the current score, and the controller input of the game. This has allowed us to use original Atari games to train and evaluate our agents. We currently support 7 Atari platformers in our experimental setup:

%\begin{multicols}{2}
%\begin{enumerate} [topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
%\item Montezumas Revenge
%\item Kung-Fu Master
%\item Frostbite
%\item Kangaroo
%\item Pitfall!
%\item Pitfall! 2
%\item H.E.R.O.
%\end{enumerate}
%\end{multicols}

\subsection{Learning Algorithms}

\begin{algorithm}[tb]
   \caption{Neural Q-Network with Experience Replay}
   \label{alg:q-learn}
\begin{algorithmic}
   \STATE Initialize Neural Q-Network with random weights
   \STATE Initialize Experience Pool to $\{ \}$
   \FOR{episode$ = 1, m$}
   \STATE Initialize previous state $s_0$ and previous action $a_0$ to NULL
   \FOR{$t = 1, T$}
   \STATE Observe state $s_t$ from the emulator
   \STATE With probability $\epsilon_a$, select random action $a_t$
   \STATE Else, set $a_i = \max_a Q(s_t, a)$ by forward propagating $s_t$ through Q
   \IF{$s_{t-1}$ and $a_{t-1}$ are not NULL}
   \STATE Observe reward $r$ from emulator
   \STATE With probability $\epsilon_r$, store the experience $\{s_{t-1}, a_{t-1}, r, s_t, a_t \}$ in the pool
   \ENDIF
   \FOR{re-experience$ = 1, ex$}
   \STATE Randomly sample experience $\{s'_0, a'_0, r', s'_1, a'_1 \}$ from the pool
    \IF{using SARSA update rule}
   \STATE Compute $v = r' + \gamma Q(s'_1, a'_1)$
   \ELSE
   \STATE Compute $v = r' + \gamma \max_a Q(s'_1, a)$
   \ENDIF
   \STATE Update Q through backpropagation of value v on output $a'_0$ with state $s'_0$
   \ENDFOR
   \STATE Apply action $a_i$ to emulator
   \STATE Update state $s_{t-1} = s_t$ and action $a_{t-1} = a_t$
   \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{State Representation in Mario}
It is challenging to find an effective representation of the Mario game state that enables effective Q-learning. The environment observations provided by Generalized Mario contains a wealth of symbolic information, but there are many possible encodings that we are investigating. Currently, our representation is a tiled grid of integers, 20x12 tiles in size. The relative value in each tile is given as a ``measure of goodness'', enemies are -2, obstacles are -1, coins are +2, etc. The grid is centered on Mario's current location. We intend to represent the state with separate substrates for each class of objects, as in \cite{Hauskneck13}, by breaking it out into separate background, enemy, reward, and actor layers. We hope to find that this representation will be universal across platform games. None of these representations encode the important velocity information about the actors, which is important in platformers where the characters move with some inertia.

\subsubsection{Neural Q-Learning}

Typically, Q-Learning approaches use a table representing the Q-function. For very large state/action spaces such as platforming games, this is impractical as the space would take too many trials to explore and converge on an optimal policy. Even using optimizations such as nearest neighbor ran out of memory for our Generalized Mario state representation. We have implemented a neural-network based Q-learning algorithm (see Algorithm~\ref{alg:q-learn}) to allow us to learn on large state/action spaces with reasonable memory utilization by finding a useful hidden layer. 

Inspired by DeepMind's approach \cite{Mnih13}, we have avoided multiple forward propagation steps when selecting optimal actions by using only the state as the input to the network, and a weight for each action as the output. Thus, we can select an optimal action for a state by propagating once, and selecting the max argument from the outputs. Also like DeepMind, we include an Experience Replay step that stores a history of events to re-learn. This avoids overfitting to the current situation and unlearning good behavior from earlier episodes. We are actively investigating methods to store experiences more intelligently. Our algorithm can optionally use the standard Q-Learning update, or the SARSA calculation.

 \begin{figure}
 \begin{center}
\includegraphics[scale=0.42]{progress_report.png}
\caption{Mario agent trained with a neural q-network with a hidden layer of 126 nodes. The agent was trained for 1000 episodes of the same level seed 3 and difficulty 1. The initial exploration factor was 1.0, and this decreased by 0.05 every 100 episodes, until stopping at 0.1. The total reward gained by the agent was summed over each episode, and the running average of 100 episodes is shown here.}
\end{center}
\end{figure}

\subsubsection{Extension to Atari Platformers}
We have setup the ALE environment, and connected default agents to it. However, we have yet to attempt to port our Mario agents to these problems. Initially, we will use 3 colored pixel substrates as the state representation, as in \cite{Hauskneck13}, but we hope to reconstruct an object layer as well.
 
 \section{Results}
 
We have found that the initial random weights in the neural network can make the early policies very flawed. To counter this, we use heavy exploration bias ($\epsilon ~ 1.0$) for early episodes, while transitioning to exploitation policies ($\epsilon$ ~ 0.1) at later episodes. We have also biased the random action to prefer motion to the right, helping the agent to explore more of the game. We trained agents over 1000 episodes in the Mario environment using the state encoded in the format described in section 2.2.1, and using the Neural Q-Network with experience replay of Algorithm~\ref{alg:q-learn}. We use Mario level of difficulty 1 so that the levels provide a challenge with enemies, but not so hard that a random agent cannot make considerable progress.
 
\section*{Acknowledgments} 
 
None.

\bibliography{final_report}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
