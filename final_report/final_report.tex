%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most modern people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% use formatting for item enumeration
\usepackage{enumitem}

\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2014} 


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Kotfis, Li, Ouyang}

\begin{document} 

\twocolumn[
\icmltitle{BroadMind: A Better Platforming Agent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Dave Kotfis}{dkotfis@seas.upenn.edu}
\icmlauthor{Zhi Li}{zhili@seas.upenn.edu}
\icmlauthor{Yesha Ouyang}{yesha@wharton.upenn.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{machine learning, reinforcement learning, deep learning}

]

\begin{abstract} 
Recent work in reinforcement learning has focused on building generalist video game agents, as opposed to focusing on a particular genre of games. We aim to build a more specialized agent focused on the more challenging genre of platform games, which has received less attention but quite difficult due to complex dynamics and partially observable game state. Utilizing symbolic representations of game state, we have trained fully connected Neural Q-Network agents to successfully learn to play a game with long term rewards and complex dynamics.
\end{abstract} 

\section{Background}
Platform games involve a free-running avatar that jumps between suspended platforms and avoid obstacles to advance through levels in the game. As a result of the large variety of environments to parse and the huge decision space, these games are very difficult for learning agents to play well. We have built an agent that does not have to simulatenously comprehend the screen-space pixels of the game and decide optimal policies. Instead, we have decoupled these two problems, using symbolic state representations from the environment encoded for the agent. However, this symbolic representation is still very large, motivating us to utilize neural network-based Q-learning approaches. 

\section{Approach}

\subsection{Experimental Setup}

We have developed a Neural Q-Network algorithm that can be extended as a reinforcement learning agent in multiple gaming environments including Generalized Mario and the Arcade Learning Environment (ALE). We utilize the RL-Glue framework to allow our agents and experiments to be used across these environments.

RL-Glue is a socket-based API that enables reinforcement learning experiments to work with software across multiple languages \cite{Tanner09}. It allows our experiments to connect reusable Python agents across many open source game environments written in languages including C++ and Java. It also enables us to customize our experiments, such as optional game visualization, loading and saving trained policies, adjusting the game difficulty, etc.

 \begin{figure}
 \begin{center}
\includegraphics[scale=0.19]{main_figure.png}
\end{center}
\end{figure}

We have trained our agents in the Generalized Mario environment. This is part of the 2009 AI Competition software package and is RL-Glue compatible \cite{Togelius10}. The Generalized Mario game has a total control input space of 12, which we encode to integers. The raw state is made up of left/right/none motions, on/off for jump, and on/off for run. The observation interface provides the 2D screen position and velocity of the Mario actor, as well as a 22x16 tile grid semantically describing the screen space with the location of coins, pipes, blocks, etc. Separately, it has a list of all enemy positions, with similar position and velocity information as provided about Mario. We use these high-level representations of game state to encode a symbolic state representation.

The Generalized Mario environment provides a positive reward of $1$ for smashing enemies or collecting coins, a substantial positive reward of $~100$ for reaching the goal, a substantial negative reward of $-10$ for Mario's death, and a small negative reward of $-0.01$ every timestep where nothing else occurred, 

We have leveraged the Arcade Learning Environment, an RL-Glue compatible framework built on top of an Atari 2600 emulator \cite{bellemare13}.  It provides an API for interfacing with the raw pixels, the current score, and the controller input of the game. This environment does not provide any symbolic state representations, so any agent would need to be capable of encoding the game state from raw pixel values. ALE has enabled us to use original Atari games to train and evaluate our agents. We currently support 7 Atari platformers in our experimental setup:

\begin{multicols}{2}
\begin{enumerate} [topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item Montezumas Revenge
\item Kung-Fu Master
\item Frostbite
\item Kangaroo
\item Pitfall!
\item Pitfall! 2
\item H.E.R.O.
\end{enumerate}
\end{multicols}

\subsection{Learning Algorithms}

 \begin{figure}
 \begin{center}
\includegraphics[scale=0.35]{mario_screenshot.png}
\caption{Raw pixels representing the state of the Mario game, as it would be seen by a player.}
\includegraphics[scale=1.2]{game_state2.png}
\caption{The equiivalent game state encoded as symbolic characters, as provided to the learning agent by the Mario environment in each timestep.}
\includegraphics[scale=0.43]{encoded_state.png}
\caption{Our encoding of the game state in a 20x12 grid with Mario at the center, as provided to the Neural Q-Network. Larger positive values correspond to positive symbols such as coins and free space, and negative values correspond to negative symbols such as enemies and obstacles.}
\end{center}
\end{figure}

\subsubsection{State Representation in Mario}
It is challenging to find an effective representation of the Mario game state that enables effective Q-learning. The environment observations provided by Generalized Mario contains a wealth of symbolic information, but there are many possible encodings that we have investigated. Our primary representation is a tiled grid of integers, 20x12 tiles in size. The relative value in each tile is given as a ``measure of goodness'', enemies are -2, obstacles are -1, coins are +2, etc. The grid is centered on Mario's current location. 

We have also evaluated an alternative representation of the state with separate substrates for each class of objects, as in \cite{Hauskneck13}, by breaking it out into separate background, enemy, and reward layers. To conserve memory and computation, this representation restricts each of the 3 substrates to an 11x7 set of tiles centered around Mario, which makes the total state size roughly equivalent to the previous representation. Note that this representation does not explicitly encode Mario's position, but it does so implicitly as the tiles are centered at Mario's position. This position will never fluctuate with tiles of odd edge lengths, so an implicit encoding should be sufficient.

It is worth nothing that none of these representations encode the important velocity information about the actors, which is important in platformers where the characters move with some inertia.

\begin{algorithm}[tb]
   \caption{Neural Q-Network with Impactful Experience Replay}
   \label{alg:q-learn}
\begin{algorithmic}
   \STATE Initialize Neural Q-Network with random weights
   \STATE Initialize Experience Pool to $\{ \}$
   \FOR{episode$ = 1, m$}
   \STATE Initialize previous state $s_0$ and previous action $a_0$ to NULL
   \FOR{$t = 1, T$}
   \STATE Compute the mean and standard deviation of absolute value of the rewards in the experience pool, $r_{ave}, r_{dev}$.
   \STATE Observe state $s_t$ from the emulator
   \STATE With probability $\epsilon_a$, select random action $a_t$
   \STATE Else, set $a_i = \max_a Q(s_t, a)$ by forward propagating $s_t$ through Q
   \IF{$s_{t-1}$ and $a_{t-1}$ are not NULL}
   \STATE Observe reward $r$ from emulator
   \STATE With probability $\epsilon_r * (1 + \frac{|r| - r_{ave}}{r_{dev}})$, store the experience $\{s_{t-1}, a_{t-1}, r, s_t, a_t \}$ in the pool
   \ENDIF
   \FOR{re-experience$ = 1, ex$}
   \STATE Randomly sample experience $\{s'_0, a'_0, r', s'_1, a'_1 \}$ from the pool
    \IF{using SARSA update rule}
   \STATE Compute $v = r' + \gamma Q(s'_1, a'_1)$
   \ELSE
   \STATE Compute $v = r' + \gamma \max_a Q(s'_1, a)$
   \ENDIF
   \STATE Update Q through backpropagation of value v on output $a'_0$ with state $s'_0$
   \ENDFOR
   \STATE Apply action $a_i$ to emulator
   \STATE Update state $s_{t-1} = s_t$ and action $a_{t-1} = a_t$
   \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Neural Q-Learning}

Typically, Q-Learning approaches use a table representing the Q-function. For very large state/action spaces such as platforming games, this is impractical as the space would take too many trials to explore and converge on an optimal policy. Even using optimizations such as nearest neighbor ran out of memory for our Generalized Mario state representation. We have implemented a neural-network based Q-learning algorithm (see Algorithm~\ref{alg:q-learn}) to allow us to learn on large state/action spaces with reasonable memory utilization by finding a useful hidden layer. 

Inspired by DeepMind's approach \cite{Mnih13}, we have avoided multiple forward propagation steps when selecting optimal actions by using only the state as the input to the network, and a weight for each action as the output. Thus, we can select an optimal action for a state by propagating once, and selecting the max argument from the outputs. Also like DeepMind, we include an Experience Replay step that stores a history of events to re-learn. This avoids overfitting to the current situation and unlearning good behavior from earlier episodes. Our algorithm can optionally use the standard Q-Learning update, or the SARSA calculation.

We have adjusted the experience remembrance process to prioritize memorization of meaningful experiences. We observe that experiences corresponding with large positive or negative rewards are rare, but these are the experiences that provide the most value to the agent. To do this, we modify the probability of remembrance, multiplying by $1 + \frac{|r| - r_{ave}}{r_{dev}}$ to increase the chances of remembering experiences that are observed to be impactful.

 \begin{figure}
 \begin{center}
\includegraphics[scale=0.28]{2layer3000.png}
\caption{Mario agent trained with a neural q-network with a hidden layer of 126 nodes. The agent was trained for 3000 episodes of the same level seed 3 and difficulty 1. The initial exploration factor was 1.0, and this decreased by 0.05 every 100 episodes, until stopping at 0.1. The total reward gained by the agent was summed over each episode, and the running average of 100 episodes is shown here.}
\includegraphics[scale=0.25]{layer2_126_550144_3000.png}
\caption{The same test performed as the previous figure, though now with 2 hidden layers. The additional layer allows the agent to achieve a higher performing agent at steady state.}
\end{center}
\end{figure}

\subsubsection{Extension to Atari Platformers}
We have setup the ALE environment, and connected default agents to it. However, we have yet to attempt to port our agents to these problems. First, we will need to develop an agent that can learn our symbolic state representations from raw pixel inputs provided by the ALE environment.
 
 \section{Results}
 
We have found that the initial random weights in the neural network can make the early policies very flawed. To counter this, we use heavy exploration bias ($\epsilon = 1.0$) for early episodes, while transitioning to exploitation policies ($\epsilon$ = 0.1) at later episodes. We have also biased the random action to prefer motion to the right, helping the agent to explore more of the game. 

We have trained agents for 1,000 episodes in the Mario environment using the state encoded in the format described in section 2.2.1, and using the Neural Q-Network with experience replay of Algorithm~\ref{alg:q-learn}. We use Mario level of difficulty 1 so that the levels provide a challenge with enemies, but not so hard that a random agent cannot make considerable progress.
 
Due to the variation in available rewards between each Mario level, we have compared all performance results against an agent that acts randomly. To reduce noise in our plots, we compute a running average of total rewards over 100 episodes. The Mario environment provides a substantially large reward for reaching the end of the level. When averaging this reward over 100 levels, it provides too much weight that de-emphasizes the results of the other episodes. For this reason, we have capped the max reward that a single episode can add to the running average.
 
 \begin{figure}
 \begin{center}
 \includegraphics[scale=0.42]{10000epsSeed8.png}
\caption{Level seed 8 continued out to 10,000 episodes with and without impact-weighted experience remembrance, all with a single hidden layer. Each point represents an average over 100 episodes.}
 \end{center}
 \end{figure}
 
 \subsection{Impactful Experience remembrance}
 We evaluated the performance of an agent prioritizing impactful experiences in their memory pool against one that equally values all new experiences. We have found that both agents saturate at the same level of performance, though the impact-weighted experiences causes that agent to learn more slowly in earlier episodes. We suspect that the weight term makes most experiences nearly impossible to remember, and this slows down the rate at which the experience pool can be filled. A good compromise may be to deactivate the weight term until the experience pool is at max capacity.
 
 \subsection{Transfer Learning Between Levels}
 
 Our initial experiments focused on simultaneous training and testing of an agent that is continuously executing the same level of the game. We are interested in training a generalist agent that can perform well on an experience that has not yet been seen. We have tested the performance of an agent that is trained for 100 episodes on each level before switching to a completely new level of similar difficulty. We have found that performance on these new levels seems to match what we would see if the agent were trained only on that level.
 
  \begin{figure}
 \begin{center}
\includegraphics[scale=0.42]{seed3wSubstrates.png}
\caption{Mario agent on level seed 3, using the multiple substrate state representation. The performance of this agent does not reach a point where it exceeds the performance of what could be achieved by a random agent.}
\end{center}
\end{figure}
 
 \subsection{Multiple Substrate Representation}
 We have trained an agent using the multiple substrate representation with separate background, enemy, and reward layers. However, we have found that this agent does not appear to be learning as its performance remains roughly in the realm of a random agent over 1000 training episodes. We believe that the smaller grid size of these substrates is not suitable for the dynamics of the Mario game.
 
 \section{Conclusion}
 
Our results have shown that a Q-Learning agent with a single hidden-layer neural network Q-function can learn to play a game with complex dynamics and partially observed states like Mario. This agent also performs well on unseen levels. 

We have found that encoding the symbolic game state in a single layer has led to agents capable of improving their performance, but splitting out the state into separate layers for background, enemies, and rewards does not exceed random performance. This may suggest that a large grid size of the state representation is important to allow the agent to plan over a longer horizon.
 
An experience replay mechanism is critical to ensure that the agent does not unlearn behavior. We have developed a novel method for emphasizing impactful experiences in memory. This approach may prove to be important when scaling this approach to a lifelong learning application. It should alleviate the need to increase the memory pool size by ensuring that less valuable experiences do not need to be stored. However, our early results have shown that this mechanism delays learning in early episodes as it takes longer to fill the experience pool.
 
Future work should focus on application of this algorithm to platforming games on a full console emulator like the Atari ALE environment where the symbolic state representation is not directly observable by the agent, and mapping raw screen space pixels into the symbolic representation must be learned. 

We have also begun to investigate the use of GPU accelerated computing to speed up the execution of agents using deep neural networks of 2 or more hidden layers.
 
 \begin{figure}
\begin{center}
\includegraphics[scale=0.72]{transfer_levels2.png}
\caption{Here, every 100 episodes the Mario environment changes to a new level. For levels 3 and 8 that we previously examined, the performance of the learned agent is consistent with what we observed in tests that trained an agent only on these levels. After the first level where the agent is learning but still acting completely randomly, the learned agent exceeds random performance in nearly every case.}
\end{center}
\end{figure}
 
\section*{Acknowledgments} 
 
None.

\bibliography{final_report}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
